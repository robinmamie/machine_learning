{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "from expansion import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from proj1_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pri0_to_drop = [\"DER_deltaeta_jet_jet\",\"DER_mass_jet_jet\",\"DER_prodeta_jet_jet\",\"DER_lep_eta_centrality\",\"PRI_jet_leading_pt\",\"PRI_jet_leading_eta\",\"PRI_jet_leading_phi\",\"PRI_jet_subleading_pt\",\"PRI_jet_subleading_eta\",\"PRI_jet_subleading_phi\"]\n",
    "pri1_to_drop = [\"DER_deltaeta_jet_jet\",\"DER_mass_jet_jet\",\"DER_prodeta_jet_jet\",\"DER_lep_eta_centrality\",\"PRI_jet_subleading_pt\",\"PRI_jet_subleading_eta\",\"PRI_jet_subleading_phi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'CClass' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-7fb25a6a467d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_TRAIN_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtx_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mhb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mhb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'CClass' object is not callable"
     ]
    }
   ],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv' # TODO: download train data and supply path here \n",
    "hb = pd.read_csv(DATA_TRAIN_PATH, sep=',')\n",
    "pd.options.display.max_columns = None\n",
    "y_orig, x_orig, _ = load_csv_data(DATA_TRAIN_PATH)\n",
    "tx_orig = np.c_(x_orig, np.ones(x_orig.shape[0]).T)\n",
    "hb = hb.drop(['Id'], 1)\n",
    "hb.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = hb[hb.DER_mass_MMC != -999].Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "g = sns.catplot(data=hb[hb.DER_mass_MMC != -999], x='Prediction', kind='count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDataSet(dataset):\n",
    "    #dataset = add_DER_mass_indicator(dataset)\n",
    "    dataset_mass_def = dataset[dataset.DER_mass_MMC != -999].copy()\n",
    "    dataset_mass_not_def = dataset[dataset.DER_mass_MMC == -999].copy()\n",
    "    dataset_mass_not_def = dataset_mass_not_def.drop(['DER_mass_MMC'],1)\n",
    "    def splitOnJetNum(dataset, DER_mass_MMC_is_defined):\n",
    "        dataset = dataset.replace(-999, np.nan)\n",
    "        if(DER_mass_MMC_is_defined):\n",
    "            dataset = nonPolyFeatureExpansion(dataset)\n",
    "\n",
    "\n",
    "        pri0 = dataset[dataset.PRI_jet_num==0].copy()\n",
    "        pri0 = pri0.drop(pri0_to_drop,1)\n",
    "        pri0 = pri0.drop([\"PRI_jet_num\",\"PRI_jet_all_pt\"],1)\n",
    "\n",
    "        pri1 = dataset[dataset.PRI_jet_num == 1].copy()\n",
    "        pri1 = pri1.drop(pri1_to_drop,1)\n",
    "        pri1 = pri1.drop([\"PRI_jet_num\"],1)\n",
    "\n",
    "        pri2 = dataset[dataset.PRI_jet_num == 2].copy()\n",
    "        pri2 = pri2.drop([\"PRI_jet_num\"],1)\n",
    "\n",
    "        pri3 = dataset[dataset.PRI_jet_num == 3].copy()\n",
    "        pri3 = pri3.drop([\"PRI_jet_num\"],1)\n",
    "\n",
    "        return [pri0,pri1,pd.concat([pri2,pri3])]\n",
    "    \n",
    "    return splitOnJetNum(dataset_mass_def, True) + splitOnJetNum(dataset_mass_not_def, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractPredictions(dataset):\n",
    "    return dataset.Prediction.apply(lambda x: -1 if x == 'b' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_DER_mass_indicator(data):\n",
    "    data['DER_mass_MMC_present'] = (data.DER_mass_MMC == -999).apply(lambda x : 1 if x else 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonPolyFeatureExpansion(data):\n",
    "    data['Mass_let_tau_sum_pt_ratio'] = data.DER_mass_MMC * data.DER_pt_ratio_lep_tau / (data.DER_sum_pt+1e-10)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataset(dataset):\n",
    "    dataset = (dataset - dataset.mean()) / dataset.std()\n",
    "    dataset = dataset.fillna(0)\n",
    "    dataset = (dataset - dataset.mean()) / dataset.std()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeDataset_numpy(dataset):\n",
    "    dataset = (dataset - dataset.mean(axis=0)) / dataset.std(axis=0)\n",
    "    dataset = np.nan_to_num(dataset)\n",
    "    dataset = (dataset - dataset.mean(axis=0)) / dataset.std(axis = 0)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tildaNumpy(X):\n",
    "    return np.c_[np.ones(X.shape[0]), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLYNOMIAL_EXPANSION_DEGREE = 13\n",
    "\n",
    "pri = cleanDataSet(hb)\n",
    "predictions = []\n",
    "pri_cross_validation_test = []\n",
    "prediction_cross_validation_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>DER_sum_pt</th>\n",
       "      <th>DER_pt_ratio_lep_tau</th>\n",
       "      <th>DER_met_phi_centrality</th>\n",
       "      <th>PRI_tau_pt</th>\n",
       "      <th>PRI_tau_eta</th>\n",
       "      <th>PRI_tau_phi</th>\n",
       "      <th>PRI_lep_pt</th>\n",
       "      <th>PRI_lep_eta</th>\n",
       "      <th>PRI_lep_phi</th>\n",
       "      <th>PRI_met</th>\n",
       "      <th>PRI_met_phi</th>\n",
       "      <th>PRI_met_sumet</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "      <th>Mass_let_tau_sum_pt_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>0.879</td>\n",
       "      <td>1.414</td>\n",
       "      <td>42.014</td>\n",
       "      <td>2.039</td>\n",
       "      <td>-3.011</td>\n",
       "      <td>36.918</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.103</td>\n",
       "      <td>44.704</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>46.226</td>\n",
       "      <td>1.130289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>s</td>\n",
       "      <td>154.916</td>\n",
       "      <td>10.418</td>\n",
       "      <td>94.714</td>\n",
       "      <td>29.169</td>\n",
       "      <td>2.897</td>\n",
       "      <td>1.526</td>\n",
       "      <td>138.178</td>\n",
       "      <td>0.365</td>\n",
       "      <td>-1.305</td>\n",
       "      <td>78.800</td>\n",
       "      <td>0.654</td>\n",
       "      <td>1.547</td>\n",
       "      <td>28.740</td>\n",
       "      <td>0.506</td>\n",
       "      <td>-1.347</td>\n",
       "      <td>22.275</td>\n",
       "      <td>-1.761</td>\n",
       "      <td>187.299</td>\n",
       "      <td>30.638</td>\n",
       "      <td>-0.715</td>\n",
       "      <td>-1.724</td>\n",
       "      <td>30.638</td>\n",
       "      <td>0.409214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>s</td>\n",
       "      <td>128.053</td>\n",
       "      <td>88.941</td>\n",
       "      <td>69.272</td>\n",
       "      <td>193.392</td>\n",
       "      <td>1.609</td>\n",
       "      <td>28.859</td>\n",
       "      <td>255.123</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.538</td>\n",
       "      <td>54.646</td>\n",
       "      <td>-1.533</td>\n",
       "      <td>0.416</td>\n",
       "      <td>32.742</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.636</td>\n",
       "      <td>132.678</td>\n",
       "      <td>0.845</td>\n",
       "      <td>294.741</td>\n",
       "      <td>167.735</td>\n",
       "      <td>-2.767</td>\n",
       "      <td>-2.514</td>\n",
       "      <td>167.735</td>\n",
       "      <td>0.300654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>b</td>\n",
       "      <td>145.297</td>\n",
       "      <td>64.234</td>\n",
       "      <td>103.565</td>\n",
       "      <td>106.999</td>\n",
       "      <td>2.183</td>\n",
       "      <td>24.660</td>\n",
       "      <td>192.245</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.689</td>\n",
       "      <td>62.890</td>\n",
       "      <td>-0.766</td>\n",
       "      <td>-1.632</td>\n",
       "      <td>36.237</td>\n",
       "      <td>0.722</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>43.910</td>\n",
       "      <td>-1.907</td>\n",
       "      <td>232.362</td>\n",
       "      <td>93.117</td>\n",
       "      <td>-0.970</td>\n",
       "      <td>1.943</td>\n",
       "      <td>93.117</td>\n",
       "      <td>0.435335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>b</td>\n",
       "      <td>114.256</td>\n",
       "      <td>4.351</td>\n",
       "      <td>67.963</td>\n",
       "      <td>47.221</td>\n",
       "      <td>2.954</td>\n",
       "      <td>26.243</td>\n",
       "      <td>100.930</td>\n",
       "      <td>1.145</td>\n",
       "      <td>0.218</td>\n",
       "      <td>30.145</td>\n",
       "      <td>0.484</td>\n",
       "      <td>-0.929</td>\n",
       "      <td>34.522</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>1.941</td>\n",
       "      <td>41.899</td>\n",
       "      <td>2.055</td>\n",
       "      <td>191.568</td>\n",
       "      <td>36.263</td>\n",
       "      <td>-0.766</td>\n",
       "      <td>-0.686</td>\n",
       "      <td>36.263</td>\n",
       "      <td>1.296177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249981</td>\n",
       "      <td>b</td>\n",
       "      <td>87.390</td>\n",
       "      <td>0.299</td>\n",
       "      <td>40.546</td>\n",
       "      <td>244.825</td>\n",
       "      <td>0.760</td>\n",
       "      <td>30.181</td>\n",
       "      <td>347.795</td>\n",
       "      <td>0.728</td>\n",
       "      <td>1.017</td>\n",
       "      <td>61.251</td>\n",
       "      <td>-0.816</td>\n",
       "      <td>-2.326</td>\n",
       "      <td>44.618</td>\n",
       "      <td>-1.542</td>\n",
       "      <td>-2.548</td>\n",
       "      <td>140.054</td>\n",
       "      <td>-2.544</td>\n",
       "      <td>449.589</td>\n",
       "      <td>241.926</td>\n",
       "      <td>-1.621</td>\n",
       "      <td>0.528</td>\n",
       "      <td>241.926</td>\n",
       "      <td>0.182924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249987</td>\n",
       "      <td>b</td>\n",
       "      <td>140.080</td>\n",
       "      <td>29.729</td>\n",
       "      <td>83.393</td>\n",
       "      <td>7.918</td>\n",
       "      <td>2.673</td>\n",
       "      <td>38.975</td>\n",
       "      <td>133.039</td>\n",
       "      <td>0.370</td>\n",
       "      <td>-1.347</td>\n",
       "      <td>70.410</td>\n",
       "      <td>-1.732</td>\n",
       "      <td>-2.851</td>\n",
       "      <td>26.085</td>\n",
       "      <td>-1.769</td>\n",
       "      <td>-0.178</td>\n",
       "      <td>47.083</td>\n",
       "      <td>0.698</td>\n",
       "      <td>185.743</td>\n",
       "      <td>36.544</td>\n",
       "      <td>-2.644</td>\n",
       "      <td>1.017</td>\n",
       "      <td>36.544</td>\n",
       "      <td>0.389582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249988</td>\n",
       "      <td>b</td>\n",
       "      <td>89.509</td>\n",
       "      <td>86.277</td>\n",
       "      <td>79.870</td>\n",
       "      <td>56.636</td>\n",
       "      <td>1.709</td>\n",
       "      <td>23.499</td>\n",
       "      <td>154.023</td>\n",
       "      <td>0.885</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>49.055</td>\n",
       "      <td>-1.040</td>\n",
       "      <td>0.142</td>\n",
       "      <td>43.415</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-1.016</td>\n",
       "      <td>43.221</td>\n",
       "      <td>1.944</td>\n",
       "      <td>175.444</td>\n",
       "      <td>61.554</td>\n",
       "      <td>-2.851</td>\n",
       "      <td>2.933</td>\n",
       "      <td>61.554</td>\n",
       "      <td>0.514309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249991</td>\n",
       "      <td>s</td>\n",
       "      <td>133.457</td>\n",
       "      <td>77.540</td>\n",
       "      <td>88.989</td>\n",
       "      <td>69.650</td>\n",
       "      <td>2.484</td>\n",
       "      <td>2.490</td>\n",
       "      <td>166.396</td>\n",
       "      <td>0.629</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>58.596</td>\n",
       "      <td>-0.834</td>\n",
       "      <td>-0.711</td>\n",
       "      <td>36.831</td>\n",
       "      <td>-1.172</td>\n",
       "      <td>1.749</td>\n",
       "      <td>41.870</td>\n",
       "      <td>-1.073</td>\n",
       "      <td>203.174</td>\n",
       "      <td>70.969</td>\n",
       "      <td>-1.234</td>\n",
       "      <td>2.521</td>\n",
       "      <td>70.969</td>\n",
       "      <td>0.504486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249997</td>\n",
       "      <td>s</td>\n",
       "      <td>105.457</td>\n",
       "      <td>60.526</td>\n",
       "      <td>75.839</td>\n",
       "      <td>39.757</td>\n",
       "      <td>2.390</td>\n",
       "      <td>22.183</td>\n",
       "      <td>120.462</td>\n",
       "      <td>1.202</td>\n",
       "      <td>0.529</td>\n",
       "      <td>35.636</td>\n",
       "      <td>-0.266</td>\n",
       "      <td>-3.132</td>\n",
       "      <td>42.834</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.851</td>\n",
       "      <td>23.419</td>\n",
       "      <td>-2.890</td>\n",
       "      <td>198.907</td>\n",
       "      <td>41.992</td>\n",
       "      <td>1.800</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>41.992</td>\n",
       "      <td>1.052276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69982 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Prediction  DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  \\\n",
       "1               b       160.937                       68.768       103.235   \n",
       "7               s       154.916                       10.418        94.714   \n",
       "9               s       128.053                       88.941        69.272   \n",
       "12              b       145.297                       64.234       103.565   \n",
       "16              b       114.256                        4.351        67.963   \n",
       "...           ...           ...                          ...           ...   \n",
       "249981          b        87.390                        0.299        40.546   \n",
       "249987          b       140.080                       29.729        83.393   \n",
       "249988          b        89.509                       86.277        79.870   \n",
       "249991          s       133.457                       77.540        88.989   \n",
       "249997          s       105.457                       60.526        75.839   \n",
       "\n",
       "        DER_pt_h  DER_deltar_tau_lep  DER_pt_tot  DER_sum_pt  \\\n",
       "1         48.146               3.473       2.078     125.157   \n",
       "7         29.169               2.897       1.526     138.178   \n",
       "9        193.392               1.609      28.859     255.123   \n",
       "12       106.999               2.183      24.660     192.245   \n",
       "16        47.221               2.954      26.243     100.930   \n",
       "...          ...                 ...         ...         ...   \n",
       "249981   244.825               0.760      30.181     347.795   \n",
       "249987     7.918               2.673      38.975     133.039   \n",
       "249988    56.636               1.709      23.499     154.023   \n",
       "249991    69.650               2.484       2.490     166.396   \n",
       "249997    39.757               2.390      22.183     120.462   \n",
       "\n",
       "        DER_pt_ratio_lep_tau  DER_met_phi_centrality  PRI_tau_pt  PRI_tau_eta  \\\n",
       "1                      0.879                   1.414      42.014        2.039   \n",
       "7                      0.365                  -1.305      78.800        0.654   \n",
       "9                      0.599                   0.538      54.646       -1.533   \n",
       "12                     0.576                   0.689      62.890       -0.766   \n",
       "16                     1.145                   0.218      30.145        0.484   \n",
       "...                      ...                     ...         ...          ...   \n",
       "249981                 0.728                   1.017      61.251       -0.816   \n",
       "249987                 0.370                  -1.347      70.410       -1.732   \n",
       "249988                 0.885                  -0.800      49.055       -1.040   \n",
       "249991                 0.629                  -0.084      58.596       -0.834   \n",
       "249997                 1.202                   0.529      35.636       -0.266   \n",
       "\n",
       "        PRI_tau_phi  PRI_lep_pt  PRI_lep_eta  PRI_lep_phi  PRI_met  \\\n",
       "1            -3.011      36.918        0.501        0.103   44.704   \n",
       "7             1.547      28.740        0.506       -1.347   22.275   \n",
       "9             0.416      32.742       -0.317       -0.636  132.678   \n",
       "12           -1.632      36.237        0.722       -0.035   43.910   \n",
       "16           -0.929      34.522       -0.215        1.941   41.899   \n",
       "...             ...         ...          ...          ...      ...   \n",
       "249981       -2.326      44.618       -1.542       -2.548  140.054   \n",
       "249987       -2.851      26.085       -1.769       -0.178   47.083   \n",
       "249988        0.142      43.415        0.216       -1.016   43.221   \n",
       "249991       -0.711      36.831       -1.172        1.749   41.870   \n",
       "249997       -3.132      42.834        0.381        0.851   23.419   \n",
       "\n",
       "        PRI_met_phi  PRI_met_sumet  PRI_jet_leading_pt  PRI_jet_leading_eta  \\\n",
       "1            -1.916        164.546              46.226                0.725   \n",
       "7            -1.761        187.299              30.638               -0.715   \n",
       "9             0.845        294.741             167.735               -2.767   \n",
       "12           -1.907        232.362              93.117               -0.970   \n",
       "16            2.055        191.568              36.263               -0.766   \n",
       "...             ...            ...                 ...                  ...   \n",
       "249981       -2.544        449.589             241.926               -1.621   \n",
       "249987        0.698        185.743              36.544               -2.644   \n",
       "249988        1.944        175.444              61.554               -2.851   \n",
       "249991       -1.073        203.174              70.969               -1.234   \n",
       "249997       -2.890        198.907              41.992                1.800   \n",
       "\n",
       "        PRI_jet_leading_phi  PRI_jet_all_pt  Mass_let_tau_sum_pt_ratio  \n",
       "1                     1.158          46.226                   1.130289  \n",
       "7                    -1.724          30.638                   0.409214  \n",
       "9                    -2.514         167.735                   0.300654  \n",
       "12                    1.943          93.117                   0.435335  \n",
       "16                   -0.686          36.263                   1.296177  \n",
       "...                     ...             ...                        ...  \n",
       "249981                0.528         241.926                   0.182924  \n",
       "249987                1.017          36.544                   0.389582  \n",
       "249988                2.933          61.554                   0.514309  \n",
       "249991                2.521          70.969                   0.504486  \n",
       "249997               -0.166          41.992                   1.052276  \n",
       "\n",
       "[69982 rows x 24 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pri[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, dataset in enumerate(pri):\n",
    "    predictions.append(extractPredictions(dataset))\n",
    "    dataset = dataset.drop(['Prediction'],1)\n",
    "    pri[idx] = tildaNumpy(normalizeDataset_numpy(polynomial_expansion( normalizeDataset(dataset).to_numpy(), POLYNOMIAL_EXPANSION_DEGREE)))\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X, w, y):\n",
    "    return (y == predict_labels(w,X)).sum()/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_e(y, tx, w):\n",
    "    return y - tx @ w\n",
    "\n",
    "def compute_loss_MSE(n2, e):\n",
    "    return (e.T @ e) / n2\n",
    "    \n",
    "def compute_gradient_MSE(tx, n, e):\n",
    "    return - tx.T @ e / n\n",
    "\n",
    "def compute_loss_MAE(n, e):\n",
    "    return 1/n * np.sum(np.abs(e))\n",
    "    \n",
    "def compute_gradient_MAE(tx, n, e):\n",
    "    return -1/n*tx.T @ np.sign(e)\n",
    "\n",
    "def compute_loss_rmse(n2, e):\n",
    "    return np.sqrt(2 * compute_loss_MSE(n2, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Linear regression using gradient descent.\n",
    "    Uses MSE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:  ndarray\n",
    "        the labels\n",
    "    tx: ndarray\n",
    "        vector x tilde, i.e. the parameters with a bias term\n",
    "    initial_w: ndarray\n",
    "        initial weight vector\n",
    "    max_iters: int\n",
    "        maximum number of iterations\n",
    "    gamma: float\n",
    "        learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss = 0\n",
    "    w = initial_w\n",
    "    n = y.shape[0]\n",
    "    n2 = n*2\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        e = compute_e(y, tx, w)\n",
    "        gradient = compute_gradient_MSE(tx, n, e)\n",
    "        loss = compute_loss_MSE(n2, e)\n",
    "        \n",
    "        # Update weights\n",
    "        w -= gamma * gradient\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}, w={w}\".format(\n",
    "        #      bi=n_iter, ti=max_iters - 1, l=loss, w=w[0]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3130589853338476\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "weights = np.array([])\n",
    "for i in range(100):\n",
    "    initial_w = np.full(tX.shape[1], i/100)\n",
    "    max_iters = 100\n",
    "    gamma = 0.3\n",
    "    w, loss = least_squares_GD(y, tX, initial_w, max_iters, gamma)\n",
    "    weights = np.append(weights, loss)\n",
    "idx = np.argmin(weights)\n",
    "'''\n",
    "initial_w = np.zeros(pri[0].shape[1])\n",
    "max_iters = 1000\n",
    "gamma = 0.001\n",
    "w, loss = least_squares_GD(predictions[0], pri[0], initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Linear regression using gradient descent.\n",
    "    Uses MAE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:  ndarray\n",
    "        the labels\n",
    "    tx: ndarray\n",
    "        vector x tilde, i.e. the parameters with a bias term\n",
    "    initial_w: ndarray\n",
    "        initial weight vector\n",
    "    max_iters: int\n",
    "        maximum number of iterations\n",
    "    gamma: float\n",
    "        learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = 0\n",
    "    w = initial_w\n",
    "    n = y.shape[0]\n",
    "    n2 = n*2\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        e = compute_e(y, tx, w)\n",
    "        gradient = compute_gradient_MAE(tx, n, e)\n",
    "        loss = compute_loss_MAE(n2, e)\n",
    "        \n",
    "        # Update weights\n",
    "        w -= gamma * gradient\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}, w={w}\".format(\n",
    "        #      bi=n_iter, ti=max_iters - 1, l=loss, w=w[0]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5287996461931668\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(pri[0].shape[1])\n",
    "max_iters = 1000\n",
    "gamma = 0.3\n",
    "w, loss = least_squares_GD(predictions[0], pri[0], initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Linear regression using stochastic gradient descent.\n",
    "    Uses MSE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:  ndarray\n",
    "        the labels\n",
    "    tx: ndarray\n",
    "        vector x tilde, i.e. the parameters with a bias term\n",
    "    initial_w: ndarray\n",
    "        initial weight vector\n",
    "    max_iters: int\n",
    "        maximum number of iterations\n",
    "    gamma: float\n",
    "        learning rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = 0\n",
    "    w = initial_w[:, np.newaxis]\n",
    "    n = y.shape[0]\n",
    "    n2 = n*2\n",
    "    data_size = len(y)\n",
    "    shuffled_indices = np.random.permutation(np.arange(data_size))\n",
    "    shuffled_y = y[shuffled_indices]\n",
    "    shuffled_tx = tx[shuffled_indices]\n",
    "    shuffled_y = shuffled_y[:,np.newaxis]\n",
    "    for n_iter, by, btx in zip(range(max_iters), shuffled_y, shuffled_tx):\n",
    "        by = by[np.newaxis]\n",
    "        btx = btx[np.newaxis, :]\n",
    "        e = compute_e(by, btx, w)\n",
    "        gradient = compute_gradient_MSE(btx, n, e)\n",
    "        loss = compute_loss_MSE(n2, e)\n",
    "        \n",
    "        # Update weights\n",
    "        w -= gamma * gradient\n",
    "        #print(\"Gradient Descent({bi}/{ti}): loss={l}, w={w}\".format(\n",
    "        #      bi=n_iter, ti=max_iters - 1, l=loss, w=w[0]))\n",
    "    return w, compute_loss_MSE(n2, compute_e(y, tx, w[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.full(pri[0].shape[1], 0.1)\n",
    "max_iters = 100000\n",
    "gamma = 0.000000001\n",
    "w, loss = least_squares_SGD(predictions[0], pri[0], initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"\n",
    "    Linear regression using normal equations.\n",
    "    Use MSE loss function\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y:  ndarray\n",
    "        the labels\n",
    "    tx: ndarray\n",
    "        vector x tilde, i.e. the parameters with a bias term\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"    \n",
    "    \n",
    "    w = la.solve(tx.T @ tx, tx.T @ y)\n",
    "    \n",
    "    return w, compute_loss_MSE(y.shape[0]*2, compute_e(y, tx, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2720999044174108\n"
     ]
    }
   ],
   "source": [
    "w, loss = least_squares(predictions[0], pri[0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge regression\n",
    "with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"\n",
    "    Ridge regression using normal equations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray\n",
    "        Description of y\n",
    "    ...\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\" \n",
    "     \n",
    "    X = tx.T @ tx\n",
    "    n = y.shape[0]\n",
    "    w = la.solve(X + lambda_ * (2 * n) * np.eye(X.shape[0]), tx.T @ y)\n",
    "    return w, compute_loss_rmse(2 * n, compute_e(y, tx, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # ***************************************************\n",
    "    # split the data based on the given ratio\n",
    "    # ***************************************************\n",
    "    N = x.shape[0]\n",
    "    indices_training = np.random.choice(N, (int)(ratio*N),replace=False)\n",
    "    mask_training = np.zeros(N, dtype=bool)\n",
    "    mask_training[indices_training] = True\n",
    "    mask_testing = ~mask_training\n",
    "    return (x[mask_training], x[mask_testing], y[mask_training], y[mask_testing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import *\n",
    "def ridge_regression_cross(x, y, ratio, seed):\n",
    "    \"\"\"ridge regression demo.\"\"\"\n",
    "    # define parameter\n",
    "    lambdas = np.logspace(-5, 0, 60)\n",
    "    # ***************************************************\n",
    "    # split the data, and return train and test data\n",
    "    # ***************************************************\n",
    "    x_train, x_test, y_train, y_test = split_data(x,y, ratio, seed)\n",
    "    \n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # ***************************************************\n",
    "        # ridge regression with a given lambda\n",
    "        # ***************************************************\n",
    "        weights, rmse = ridge_regression(y_train, x_train, lambda_)\n",
    "        rmse_tr.append(rmse)\n",
    "        rmse_te.append(compute_loss_rmse(y_test.shape[0]*2, compute_e(y_test,x_test, weights)))\n",
    "\n",
    "        print(\"proportion={p}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}\".format(\n",
    "               p=ratio, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))\n",
    "        \n",
    "    # Plot the obtained results\n",
    "    plot_train_test(rmse_tr, rmse_te, lambdas)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion=0.8, lambda=0.000, Training RMSE=0.508, Testing RMSE=0.635\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.509, Testing RMSE=0.658\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.509, Testing RMSE=0.680\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.509, Testing RMSE=0.696\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.510, Testing RMSE=0.703\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.510, Testing RMSE=0.701\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.511, Testing RMSE=0.691\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.511, Testing RMSE=0.673\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.511, Testing RMSE=0.653\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.512, Testing RMSE=0.630\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.512, Testing RMSE=0.609\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.512, Testing RMSE=0.591\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.513, Testing RMSE=0.577\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.513, Testing RMSE=0.568\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.513, Testing RMSE=0.563\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.514, Testing RMSE=0.561\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.514, Testing RMSE=0.561\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.515, Testing RMSE=0.564\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.515, Testing RMSE=0.566\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.515, Testing RMSE=0.569\n",
      "proportion=0.8, lambda=0.000, Training RMSE=0.516, Testing RMSE=0.572\n",
      "proportion=0.8, lambda=0.001, Training RMSE=0.516, Testing RMSE=0.573\n",
      "proportion=0.8, lambda=0.001, Training RMSE=0.517, Testing RMSE=0.574\n",
      "proportion=0.8, lambda=0.001, Training RMSE=0.517, Testing RMSE=0.574\n",
      "proportion=0.8, lambda=0.001, Training RMSE=0.518, Testing RMSE=0.573\n",
      "proportion=0.8, lambda=0.001, Training RMSE=0.518, Testing RMSE=0.572\n",
      "proportion=0.8, lambda=0.002, Training RMSE=0.519, Testing RMSE=0.570\n",
      "proportion=0.8, lambda=0.002, Training RMSE=0.519, Testing RMSE=0.568\n",
      "proportion=0.8, lambda=0.002, Training RMSE=0.520, Testing RMSE=0.566\n",
      "proportion=0.8, lambda=0.003, Training RMSE=0.520, Testing RMSE=0.565\n",
      "proportion=0.8, lambda=0.003, Training RMSE=0.521, Testing RMSE=0.564\n",
      "proportion=0.8, lambda=0.004, Training RMSE=0.521, Testing RMSE=0.563\n",
      "proportion=0.8, lambda=0.005, Training RMSE=0.522, Testing RMSE=0.563\n",
      "proportion=0.8, lambda=0.006, Training RMSE=0.523, Testing RMSE=0.564\n",
      "proportion=0.8, lambda=0.008, Training RMSE=0.524, Testing RMSE=0.565\n",
      "proportion=0.8, lambda=0.009, Training RMSE=0.524, Testing RMSE=0.566\n",
      "proportion=0.8, lambda=0.011, Training RMSE=0.525, Testing RMSE=0.567\n",
      "proportion=0.8, lambda=0.014, Training RMSE=0.526, Testing RMSE=0.569\n",
      "proportion=0.8, lambda=0.017, Training RMSE=0.527, Testing RMSE=0.570\n",
      "proportion=0.8, lambda=0.020, Training RMSE=0.528, Testing RMSE=0.571\n",
      "proportion=0.8, lambda=0.025, Training RMSE=0.529, Testing RMSE=0.571\n",
      "proportion=0.8, lambda=0.030, Training RMSE=0.531, Testing RMSE=0.572\n",
      "proportion=0.8, lambda=0.036, Training RMSE=0.533, Testing RMSE=0.572\n",
      "proportion=0.8, lambda=0.044, Training RMSE=0.535, Testing RMSE=0.573\n",
      "proportion=0.8, lambda=0.054, Training RMSE=0.538, Testing RMSE=0.574\n",
      "proportion=0.8, lambda=0.065, Training RMSE=0.541, Testing RMSE=0.575\n",
      "proportion=0.8, lambda=0.079, Training RMSE=0.546, Testing RMSE=0.577\n",
      "proportion=0.8, lambda=0.096, Training RMSE=0.551, Testing RMSE=0.581\n",
      "proportion=0.8, lambda=0.117, Training RMSE=0.558, Testing RMSE=0.586\n",
      "proportion=0.8, lambda=0.142, Training RMSE=0.567, Testing RMSE=0.592\n",
      "proportion=0.8, lambda=0.173, Training RMSE=0.577, Testing RMSE=0.600\n",
      "proportion=0.8, lambda=0.210, Training RMSE=0.590, Testing RMSE=0.611\n",
      "proportion=0.8, lambda=0.255, Training RMSE=0.604, Testing RMSE=0.624\n",
      "proportion=0.8, lambda=0.310, Training RMSE=0.622, Testing RMSE=0.639\n",
      "proportion=0.8, lambda=0.377, Training RMSE=0.641, Testing RMSE=0.656\n",
      "proportion=0.8, lambda=0.458, Training RMSE=0.662, Testing RMSE=0.676\n",
      "proportion=0.8, lambda=0.557, Training RMSE=0.685, Testing RMSE=0.697\n",
      "proportion=0.8, lambda=0.677, Training RMSE=0.709, Testing RMSE=0.719\n",
      "proportion=0.8, lambda=0.823, Training RMSE=0.734, Testing RMSE=0.742\n",
      "proportion=0.8, lambda=1.000, Training RMSE=0.758, Testing RMSE=0.765\n",
      "CPU times: user 8.75 s, sys: 7.86 s, total: 16.6 s\n",
      "Wall time: 3.01 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "ridge_regression_cross(pri[5], predictions[5], 0.8, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "\n",
    "    x_tr = np.delete(x, k_indices[k], axis=0)\n",
    "    y_tr = np.delete(y, k_indices[k], axis=0)\n",
    "    \n",
    "    x_te = x[k_indices[k]]\n",
    "    y_te = y[k_indices[k]]\n",
    "    w, rmse = ridge_regression(y_tr, x_tr, lambda_)\n",
    "    \n",
    "    n = y.shape[0]\n",
    "    loss_tr = rmse\n",
    "    loss_te = compute_loss_rmse(2*n , compute_e(y_te, x_te, w))\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_k_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[0;34m(i)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_k_indices' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def cross_validation_visualization_wo_error(lambdas, rmse_tr, rmse_te):\n",
    "    zeros = np.zeros(len(rmse_te))\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te, zeros, zeros)\n",
    "\n",
    "ridge_parameters = []\n",
    "    \n",
    "def cross_validation_demo(i):\n",
    "    seed = 2019\n",
    "    k_fold = 2\n",
    "    lambdas = np.logspace(-10, -3, 15)\n",
    "    if i == 0:\n",
    "        lambdas = np.logspace(-3.5, -1.5, 15)\n",
    "    elif i == 1:\n",
    "        lambdas = np.logspace(-6, -3, 15)\n",
    "    elif i == 2:\n",
    "        lambdas = np.logspace(-7, -5, 15)\n",
    "    elif i == 3:\n",
    "        lambdas = np.logspace(-3, -2, 15)\n",
    "    \n",
    "    y = predictions[i]\n",
    "    y = y[:,np.newaxis]\n",
    "    x = pri[i]\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    var_tr = []\n",
    "    var_te = []\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        loss_tr = []\n",
    "        loss_te = []\n",
    "        for k in range(k_fold):\n",
    "            tr, te = cross_validation(y, x, k_indices, k, lambda_)\n",
    "            loss_tr.append(tr)\n",
    "            loss_te.append(te)\n",
    "        rmse_tr.append(np.mean(loss_tr))\n",
    "        rmse_te.append(np.mean(loss_te))\n",
    "        var_tr.append(np.std(loss_tr))\n",
    "        var_te.append(np.std(loss_te))\n",
    "    optimal = lambdas[np.argmin(var_te)]\n",
    "    print(optimal)\n",
    "    ridge_parameters.append(optimal)\n",
    "    plt.subplot(2, 4, 1+i)\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te, var_tr, var_te)\n",
    "\n",
    "for i in range(len(pri)):\n",
    "    cross_validation_demo(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7575175948028048\n",
      "0.8069999111160885\n",
      "0.7647957383995958\n",
      "0.41754322045820225\n",
      "0.5128168808156555\n",
      "0.5190484670966447\n"
     ]
    }
   ],
   "source": [
    "w_pri = []\n",
    "for i in range(len(pri)):\n",
    "    w, loss = ridge_regression(predictions[i], pri[i], ridge_parameters[i])\n",
    "    print(loss)\n",
    "    w_pri.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1/(1 + np.exp(-t))\n",
    "    \n",
    "def compute_loss_logistic(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    return np.sum(np.log(1 + np.exp(tx @ w)) - y * (tx @ w))\n",
    "\n",
    "\n",
    "def compute_gradient_logistic(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T @ (sigmoid(tx @ w) - y)\n",
    "\n",
    "\n",
    "def logistic_regression_step(y, tx, w):\n",
    "    \"\"\"return the loss, gradient\"\"\"\n",
    "    return compute_loss_logistic(y, tx, w), compute_gradient_logistic(y, tx, w)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Logistic regression using gradient descent or SGD.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray\n",
    "        Description of y\n",
    "    ...\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"  \n",
    "    def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "        \"\"\"\n",
    "        Do one step of gradient descen using logistic regression.\n",
    "        Return the loss and the updated w.\n",
    "        \"\"\"\n",
    "        loss, gradient = logistic_regression_step(y, tx, w)\n",
    "        w -= gamma * gradient\n",
    "        return loss, w\n",
    "    \n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    # build tx\n",
    "    y = y[:,np.newaxis]\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "     \n",
    "    print(\"loss={l}\".format(l=compute_loss_logistic(y, tx, w)))\n",
    "    return w, losses[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=51145.85466827621\n",
      "Current iteration=100, loss=34890.484787313915\n",
      "Current iteration=200, loss=33641.276577518096\n",
      "Current iteration=300, loss=33160.07729551468\n",
      "Current iteration=400, loss=32871.834328063065\n",
      "Current iteration=500, loss=32668.43163422678\n",
      "Current iteration=600, loss=32513.35417963414\n",
      "Current iteration=700, loss=32389.443607726047\n",
      "Current iteration=800, loss=32287.30450649813\n",
      "Current iteration=900, loss=32201.333556455655\n",
      "Current iteration=1000, loss=32127.62579606862\n",
      "Current iteration=1100, loss=32063.393896175483\n",
      "Current iteration=1200, loss=32006.571959982197\n",
      "Current iteration=1300, loss=31955.64628308703\n",
      "Current iteration=1400, loss=31909.478357822576\n",
      "Current iteration=1500, loss=31867.221898433403\n",
      "Current iteration=1600, loss=31828.265342017068\n",
      "Current iteration=1700, loss=31792.153742643415\n",
      "Current iteration=1800, loss=31758.52572073961\n",
      "Current iteration=1900, loss=31727.085511825324\n",
      "Current iteration=2000, loss=31697.59520920833\n",
      "Current iteration=2100, loss=31669.877106742424\n",
      "Current iteration=2200, loss=31643.824662997307\n",
      "Current iteration=2300, loss=31619.37907402934\n",
      "Current iteration=2400, loss=31596.441533352194\n",
      "Current iteration=2500, loss=31574.885894325846\n",
      "Current iteration=2600, loss=31554.60389420415\n",
      "Current iteration=2700, loss=31535.498450771316\n",
      "Current iteration=2800, loss=31517.476353119604\n",
      "Current iteration=2900, loss=31500.44850758351\n",
      "Current iteration=3000, loss=31484.331717865833\n",
      "Current iteration=3100, loss=31469.049715823905\n",
      "Current iteration=3200, loss=31454.533375397175\n",
      "Current iteration=3300, loss=31440.720445838037\n",
      "Current iteration=3400, loss=31427.555059737533\n",
      "Current iteration=3500, loss=31414.9871589595\n",
      "Current iteration=3600, loss=31402.971904180653\n",
      "Current iteration=3700, loss=31391.46909149776\n",
      "Current iteration=3800, loss=31380.442586999234\n",
      "Current iteration=3900, loss=31369.85980196543\n",
      "Current iteration=4000, loss=31359.691252495428\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-f47a27808cac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minitial_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpri\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpri\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-102-f9e6877baf6a>\u001b[0m in \u001b[0;36mlogistic_regression\u001b[0;34m(y, tx, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;31m# log info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-102-f9e6877baf6a>\u001b[0m in \u001b[0;36mlearning_by_gradient_descent\u001b[0;34m(y, tx, w, gamma)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \"\"\"\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-62a75f52390d>\u001b[0m in \u001b[0;36mlogistic_regression_step\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogistic_regression_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"\"\"return the loss, gradient\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompute_loss_logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_gradient_logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-62a75f52390d>\u001b[0m in \u001b[0;36mcompute_loss_logistic\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss_logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"\"\"compute the cost by negative log likelihood.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y = (y+1)/2\n",
    "gamma = 1e-6\n",
    "initial_w = np.zeros((pri[0].shape[1], 1))\n",
    "w, loss = logistic_regression(predictions[0], pri[0], initial_w, 10000, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Regularized logistic regression using gradient descent or SGD.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : ndarray\n",
    "        Description of y\n",
    "    ...\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (ndarray, float)\n",
    "        Last weight vector and the corresponding loss value\n",
    "    \"\"\"    \n",
    "    def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "        \"\"\"return the loss, gradient\"\"\"\n",
    "        loss, gradient = logistic_regression_step(y, tx, w)\n",
    "        loss     += 2 * lambda_ * la.norm(w)**2\n",
    "        gradient += lambda_ * w\n",
    "\n",
    "        return loss, gradient\n",
    "    \n",
    "    def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "        \"\"\"\n",
    "        Do one step of gradient descent, using the penalized logistic regression.\n",
    "        Return the loss and updated w.\n",
    "        \"\"\"\n",
    "        loss, gradient = penalized_logistic_regression(y, tx, w, lambda_) \n",
    "        w = w - gamma * gradient \n",
    "\n",
    "        return loss, w\n",
    "    \n",
    "    # init parameters\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    y = (y+1)/2\n",
    "\n",
    "    w = initial_w\n",
    "    y = y[:,np.newaxis]\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iters):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "            \n",
    "    print(\"loss={l}\".format(l=compute_loss_logistic(y, tx, w)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=51145.85466827621\n",
      "Current iteration=100, loss=33636.426693549955\n",
      "Current iteration=200, loss=32870.27952016065\n",
      "Current iteration=300, loss=32512.507258851587\n",
      "Current iteration=400, loss=32286.76557299292\n",
      "Current iteration=500, loss=32127.25374746054\n",
      "Current iteration=600, loss=32006.307502541622\n",
      "Current iteration=700, loss=31909.287365296554\n",
      "Current iteration=800, loss=31828.124079379344\n",
      "Current iteration=900, loss=31758.418396475445\n",
      "Current iteration=1000, loss=31697.511949064083\n",
      "Current iteration=1100, loss=31643.756531721217\n",
      "Current iteration=1200, loss=31596.382040636454\n",
      "Current iteration=1300, loss=31554.551651151007\n",
      "Current iteration=1400, loss=31517.42993651121\n",
      "Current iteration=1500, loss=31484.290265979995\n",
      "Current iteration=1600, loss=31454.49627434351\n",
      "Current iteration=1700, loss=31427.521782967\n",
      "Current iteration=1800, loss=31402.941950477332\n",
      "Current iteration=1900, loss=31380.415532359075\n",
      "Current iteration=2000, loss=31359.666895786737\n",
      "Current iteration=2100, loss=31340.471001940703\n",
      "Current iteration=2200, loss=31322.644471311552\n",
      "Current iteration=2300, loss=31306.04158627406\n",
      "Current iteration=2400, loss=31290.54320763057\n",
      "Current iteration=2500, loss=31276.039943417953\n",
      "Current iteration=2600, loss=31262.447414585116\n",
      "Current iteration=2700, loss=31249.70766560309\n",
      "Current iteration=2800, loss=31237.806600844153\n",
      "Current iteration=2900, loss=31226.725649915883\n",
      "Current iteration=3000, loss=31216.33895510007\n",
      "Current iteration=3100, loss=31206.529207066607\n",
      "Current iteration=3200, loss=31197.237567603242\n",
      "Current iteration=3300, loss=31188.421696081477\n",
      "Current iteration=3400, loss=31180.042040984652\n",
      "Current iteration=3500, loss=31172.0614910388\n",
      "Current iteration=3600, loss=31164.446147564475\n",
      "Current iteration=3700, loss=31157.16553250148\n",
      "Current iteration=3800, loss=31150.19242119613\n",
      "Current iteration=3900, loss=31143.502530773443\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-59b949bef914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2e-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpri\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-86-5ae4e5329032>\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_penalized_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;31m# log info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-5ae4e5329032>\u001b[0m in \u001b[0;36mlearning_by_penalized_gradient\u001b[0;34m(y, tx, w, gamma, lambda_)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \"\"\"\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-5ae4e5329032>\u001b[0m in \u001b[0;36mpenalized_logistic_regression\u001b[0;34m(y, tx, w, lambda_)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpenalized_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;34m\"\"\"return the loss, gradient\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m     \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mgradient\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-62a75f52390d>\u001b[0m in \u001b[0;36mlogistic_regression_step\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogistic_regression_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"\"\"return the loss, gradient\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcompute_loss_logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_gradient_logistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-62a75f52390d>\u001b[0m in \u001b[0;36mcompute_loss_logistic\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"\"\"compute the cost by negative log likelihood.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lambda_ = 1e-6\n",
    "initial_w = np.zeros((pri[0].shape[1], 1))\n",
    "max_iters = 10000\n",
    "gamma = 2e-6\n",
    "w, loss = reg_logistic_regression(predictions[0], pri[0], lambda_, initial_w, max_iters, gamma)\n",
    "print(np.mean(predict_labels(w, )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of ML magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'least_squares_GD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-69324ad63f75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleast_squares_GD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpri\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'least_squares_GD' is not defined"
     ]
    }
   ],
   "source": [
    "initial_w = np.full(pri[0].shape[1], 0.1)\n",
    "max_iters = 600\n",
    "gamma = 0.0001\n",
    "w, loss = least_squares_GD(predictions[0].to_numpy(), pri[0], initial_w, max_iters, gamma)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'least_squares_GD' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-28eee7431fc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m          (ridge_regression, (lamb,1))]\n\u001b[1;32m     13\u001b[0m '''\n\u001b[0;32m---> 14\u001b[0;31m pri_learn_func = [(least_squares_GD, (np.zeros(pri[0].shape[1]), 1000, 0.1)),\n\u001b[0m\u001b[1;32m     15\u001b[0m          \u001b[0;34m(\u001b[0m\u001b[0mleast_squares_GD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpri\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m          \u001b[0;34m(\u001b[0m\u001b[0mleast_squares_GD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpri\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'least_squares_GD' is not defined"
     ]
    }
   ],
   "source": [
    "# methodes element of this contains [(methode_for_learning,(parameters_of_methode))]\n",
    "'''pri_learn_func = [(least_squares_GD, (np.zeros(pri[0].shape[1]), 1000, 0.1)),\n",
    "         (least_squares_GD, (np.zeros(pri[1].shape[1]), 1000, 0.1)),\n",
    "         (least_squares_GD,(np.zeros(pri[2].shape[1]), 1000, 0.1)), \n",
    "         (least_squares_GD, (np.zeros(pri[3].shape[1]), 1000, 0.1))]\n",
    "'''\n",
    "'''\n",
    "lamb = 4.64e-06\n",
    "pri_learn_func = [(ridge_regression, (lamb,1)),\n",
    "         (ridge_regression, (lamb,1)),\n",
    "         (ridge_regression,(lamb,1)), \n",
    "         (ridge_regression, (lamb,1))]\n",
    "'''\n",
    "pri_learn_func = [(least_squares_GD, (np.zeros(pri[0].shape[1]), 1000, 0.1)),\n",
    "         (least_squares_GD, (np.zeros(pri[1].shape[1]), 1000, 0.1)),\n",
    "         (least_squares_GD,(np.zeros(pri[2].shape[1]), 1000, 0.1)), \n",
    "         (least_squares_GD, (np.zeros(pri[3].shape[1]), 1000, 0.1))]\n",
    "\n",
    "\n",
    "def learn(pri, pri_learn_func):\n",
    "    w_pri = []\n",
    "    losses = []\n",
    "    for idx in range(len(pri)):\n",
    "        learning_function, parameters = pri_learn_func[idx]\n",
    "        w ,loss = learning_function(predictions[idx].to_numpy(),pri[idx],*parameters)\n",
    "        print(\"* \" + str(idx) + \" loss : \" + str(loss))\n",
    "        w_pri.append(w)\n",
    "        losses.append(loss)\n",
    "    return (w_pri, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_pri, losses = learn(pri,pri_learn_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w_pri[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_attempt(pri_cross_validation_test, w_pri, prediction_cross_validation_test):\n",
    "    res = []\n",
    "    for i in range(len(w_pri)):\n",
    "        s = score(pri_cross_validation_test[i], w_pri[i], prediction_cross_validation_test[i])\n",
    "        res.append(s)\n",
    "        print(\"Socre for pri : \" +  str(i) + \" is : \"+ str(s))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_attempt(pri_cross_validation_test, w_pri, prediction_cross_validation_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(pri_cross_validation_test[1], w_pri[1], prediction_cross_validation_test[1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mean = []\n",
    "for i in tqdm(range(0, 1000, 2)):\n",
    "    lamb = i/1000\n",
    "    pri_learn_func = [(ridge_regression, (lamb,1)),\n",
    "         (ridge_regression, (lamb,1)),\n",
    "         (ridge_regression,(lamb,1)), \n",
    "         (ridge_regression, (lamb,1))]\n",
    "    w_pri, losses = learn(pri,pri_learn_func)\n",
    "    loss_mean.append(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mean.index(np.min(loss_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " w_pri =w_pri_deg4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(t,_):\n",
    "    print(_)\n",
    "\n",
    "p(*test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "hbt = pd.read_csv(DATA_TEST_PATH, sep=',')\n",
    "\n",
    "hbt = hbt.drop(['Prediction'], 1)\n",
    "\n",
    "hbt = hbt.set_index(['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pri = cleanDataSet(hbt)\n",
    "test_pri_tX = [] # tX arrays to run prediction on\n",
    "for idx , dataset in enumerate(test_pri):\n",
    "    test_pri_tX.append( tildaNumpy(normalizeDataset_numpy(polynomial_expansion( normalizeDataset(dataset).to_numpy(), POLYNOMIAL_EXPANSION_DEGREE))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_pri,test_pri_tX, w_pri):\n",
    "    for idx, dataset in enumerate(test_pri_tX):\n",
    "        test_pri[idx]['Prediction'] = predict_labels(w_pri[idx],dataset)\n",
    "    return test_pri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = predict(test_pri,test_pri_tX,w_pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = pd.concat(test_prediction,sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = test_prediction.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'predictions.csv'\n",
    "create_csv_submission(test_prediction.Prediction.keys(), test_prediction.Prediction.values, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' # TODO: download train data and supply path here \n",
    "_, X_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "hbt = pd.read_csv(DATA_TEST_PATH, sep=',')\n",
    "hbt = hbt.drop(['Id', 'Prediction'], 1)\n",
    "hbt = hbt.replace(-999, np.nan)\n",
    "hbt = (hbt - hbt.mean()) / hbt.std()\n",
    "hbt = hbt.fillna(0)\n",
    "hbt = (hbt - hbt.mean()) / hbt.std()\n",
    "tX_test = np.c_[np.ones(X_test.shape[0]), hbt.to_numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'predictions.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = predict_labels(w[1:], tX_test)#[:, [0, 1, 2, 3, 4, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]]) # Selected desired columns\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568238\n",
      "173231\n"
     ]
    }
   ],
   "source": [
    "print(len(test_prediction.Prediction.values))\n",
    "print(len(test_prediction.Prediction.values[test_prediction.Prediction.values > 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissions scores\n",
    "Best score by technique\n",
    "\n",
    "<ul>\n",
    "    <li>MSE, gradient descent : 0.649</li>\n",
    "    <li>MAE, gradient descent : 0.678 </li>\n",
    "    <li>ridge regression      : 0.664</li>\n",
    "</ul>\n",
    "Best score after not being stupid with bias:\n",
    "\n",
    "* MSE, GD: \n",
    "* MAE, GD: 0.639\n",
    "* LSQ: 0.706\n",
    "* R-REG: 0.730\n",
    "\n",
    "Best score after normalizing test set + putting zero where unknown:\n",
    "\n",
    "* LSQ: 0.747\n",
    "* R-REG: 0.745\n",
    "\n",
    "Feature expansion?\n",
    "\n",
    "Degree polynomial 4, and jet-num separation MAE\n",
    "with these learning parameters:\n",
    "```\n",
    "pri_learn_func = [(least_squares_GD, (np.zeros(pri[0].shape[1]), 1000, 0.1)),\n",
    "         (least_squares_GD, (np.zeros(pri[1].shape[1]), 1000, 0.1)),\n",
    "         (least_squares_GD,(np.zeros(pri[2].shape[1]), 1000, 0.1)), \n",
    "         (least_squares_GD, (np.zeros(pri[3].shape[1]), 1000, 0.1))]\n",
    "```\n",
    "0.801\n",
    "\n",
    "\n",
    "Degree polynomial 9 , jet-num separation\n",
    "learning parameters:\n",
    "```\n",
    "lamb = 4.64e-06\n",
    "pri_learn_func = [(ridge_regression, (lamb,1)),\n",
    "         (ridge_regression, (lamb,1)),\n",
    "         (ridge_regression,(lamb,1)), \n",
    "         (ridge_regression, (lamb,1))]\n",
    "```\n",
    "0.779\n",
    "\n",
    "With r-regression and cross-validation, 12 degrees:\n",
    "\n",
    "- 0.81\n",
    "- With Jeremy's hack (1 more dimension for NaN mass): 0.811\n",
    "- Taking whole dataset, not 90%: 0.807\n",
    "\n",
    "Split the dataset into 8. With and without DM mass defined.\n",
    "- 0.812\n",
    "\n",
    "Split data PRI 0, 1, 2+3 and my Mass deffined\n",
    "- 0.814"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
